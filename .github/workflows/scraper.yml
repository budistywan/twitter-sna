name: ğŸ¦ Twitter Scraper

on:
  workflow_dispatch:
    inputs:
      search_query:
        description: 'Query pencarian (contoh: #indonesia, @username, kata kunci)'
        required: true
        default: '#indonesia'
      max_tweets:
        description: 'Jumlah maksimum tweet'
        required: true
        default: '200'

  schedule:
    - cron: '0 1 * * *'

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write

    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4

      - name: ğŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: ğŸ“¦ Install dependencies
        run: |
          pip install twscrape pandas networkx

      - name: ğŸ” Run scraper
        env:
          SEARCH_QUERY: ${{ github.event.inputs.search_query || '#indonesia' }}
          MAX_TWEETS:   ${{ github.event.inputs.max_tweets || '200' }}
          TW_USERNAME:  ${{ secrets.TW_USERNAME }}
          TW_PASSWORD:  ${{ secrets.TW_PASSWORD }}
          TW_EMAIL:     ${{ secrets.TW_EMAIL }}
        run: |
          python scripts/scraper.py

      - name: ğŸ“¤ Upload hasil sebagai Artifact
        uses: actions/upload-artifact@v4
        with:
          name: twitter-data-${{ github.run_number }}
          path: data/
          retention-days: 30

      - name: ğŸ’¾ Commit data ke repository
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add data/ || true
          git diff --staged --quiet || git commit -m "ğŸ“Š Data scraping: ${{ github.event.inputs.search_query || 'scheduled' }} [$(date +'%Y-%m-%d %H:%M')]"
          git push || echo "Tidak ada perubahan untuk di-push"
